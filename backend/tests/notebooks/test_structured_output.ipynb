{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "25335c50",
            "metadata": {},
            "outputs": [],
            "source": [
                "#set path\n",
                "import os\n",
                "import sys\n",
                "import dotenv\n",
                "\n",
                "dotenv.load_dotenv(override=True)\n",
                "\n",
                "paths = [\n",
                "    os.path.abspath(os.path.join(os.getcwd(), \"..\")),  # backend/\n",
                "]\n",
                "\n",
                "for path in paths:\n",
                "    if path not in sys.path:\n",
                "        sys.path.insert(0, path)\n",
                "\n",
                "from agents.llm_factory import LLMFactory, OpenAIModel\n",
                "from agents.state import ClassNode\n",
                "\n",
                "from langchain_core.messages import HumanMessage\n",
                "from langchain_openai import ChatOpenAI\n",
                "from langchain_core.output_parsers import PydanticOutputParser\n",
                "from pydantic import BaseModel, Field\n",
                "from typing import Optional"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "5f105cdf",
            "metadata": {},
            "source": [
                "OpenAI json_schema method doesn't allow dict or tuple. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "id": "a090970e",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "items=['Node1', 'Node2', 'Node3', 'Node4']\n"
                    ]
                }
            ],
            "source": [
                "llm = LLMFactory()\n",
                "class CorrectSchema(BaseModel):\n",
                "    items: list[str] = Field(\n",
                "        description=\"List of nodes to create. Leave empty if you don't think we need to create more branches.\"\n",
                "    )\n",
                "\n",
                "class WrongSchema(BaseModel):\n",
                "    items: dict[str, int] = Field(\n",
                "        description=\"List of nodes to create. Leave empty if you don't think we need to create more branches.\"\n",
                "    )\n",
                "\n",
                "\n",
                "response = await llm.ainvoke(\n",
                "    model=OpenAIModel.GPT_4O_MINI,\n",
                "    prompts=[\n",
                "        HumanMessage(\n",
                "            content=\"Just imagine anything. This is a mock test. But you should return a valid output schema.\"\n",
                "        )\n",
                "    ],\n",
                "    output_schema=CorrectSchema,\n",
                ")\n",
                "print(response)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "id": "242d83c5",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/Users/minkijung/Documents/SideProjects/self_evolving_taxonomy_agent/backend/.venv/lib/python3.13/site-packages/langchain_openai/chat_models/base.py:425: UserWarning: Invalid schema for OpenAI's structured output feature, which is the default method for `with_structured_output` as of langchain-openai==0.3. Specify `method=\"function_calling\"` instead or update your schema. See supported schemas: https://platform.openai.com/docs/guides/structured-outputs#supported-schemas\n",
                        "  warnings.warn(message)\n",
                        "Invalide Schema error with model OpenAIModel.GPT_4O_MINI: error code: 400 - {'error': {'message': \"invalid schema for response_format 'wrongschema': in context=(), 'required' is required to be supplied and to be an array including every key in properties. extra required key 'items' supplied.\", 'type': 'invalid_request_error', 'param': 'response_format', 'code': none}}\n",
                        "Failed to get valid response from OpenAIModel.GPT_4O_MINI after 4 attempts\n",
                        "Got None response from the model, meaning it failed after retries.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "None\n"
                    ]
                }
            ],
            "source": [
                "\n",
                "response = await llm.ainvoke(\n",
                "    model=OpenAIModel.GPT_4O_MINI,\n",
                "    prompts=[\n",
                "        HumanMessage(\n",
                "            content=\"Just imagine anything. This is a mock test. But you should return a valid output schema.\"\n",
                "        )\n",
                "    ],\n",
                "    output_schema=WrongSchema,\n",
                ")\n",
                "print(response)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b5e94a09",
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.2"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}